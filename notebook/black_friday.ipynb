{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Friday – Prédiction du montant d'achat\n",
    "\n",
    "## 1. Introduction & Objectif\n",
    "\n",
    "L'entreprise **ABC Private Limited** souhaite comprendre le comportement d'achat de ses clients lors du Black Friday et **prédire le montant des achats (`Purchase`)** afin de proposer des offres personnalisées.\n",
    "\n",
    "Ce notebook présente l'ensemble du pipeline :\n",
    "- Analyse exploratoire (EDA)\n",
    "- Nettoyage et feature engineering\n",
    "- Modélisation (baselines + CatBoost)\n",
    "- Interprétabilité (SHAP)\n",
    "- Clustering clients\n",
    "- Prédictions finales sur le jeu de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Configuration Google Colab\n",
    "# ============================================================\n",
    "import os, sys\n",
    "\n",
    "# Détection Colab\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Cloner le repo si nécessaire\n",
    "    if not os.path.exists(\"data-science\"):\n",
    "        !git clone https://github.com/<votre-user>/data-science.git\n",
    "    os.chdir(\"data-science\")\n",
    "    !pip install -q -r requirements.txt\n",
    "else:\n",
    "    # Exécution locale : on se place à la racine du projet\n",
    "    if os.path.basename(os.getcwd()) == \"notebook\":\n",
    "        os.chdir(\"..\")\n",
    "\n",
    "print(\"Répertoire de travail :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Imports\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(f\"Train : {train.shape}\")\n",
    "print(f\"Test  : {test.shape}\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyse Exploratoire (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes\n",
    "missing = train.isnull().sum()\n",
    "missing_pct = (missing / len(train) * 100).round(2)\n",
    "pd.DataFrame({\"Manquantes\": missing, \"%\": missing_pct}).query(\"Manquantes > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train[\"Purchase\"], bins=50, edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_title(\"Distribution de Purchase\")\n",
    "axes[0].set_xlabel(\"Purchase\")\n",
    "\n",
    "axes[1].hist(np.log1p(train[\"Purchase\"]), bins=50, edgecolor=\"black\", alpha=0.7, color=\"orange\")\n",
    "axes[1].set_title(\"Distribution de log1p(Purchase)\")\n",
    "axes[1].set_xlabel(\"log1p(Purchase)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achats par Genre\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train[\"Gender\"].value_counts().plot(kind=\"bar\", ax=axes[0], color=[\"steelblue\", \"salmon\"])\n",
    "axes[0].set_title(\"Nombre de transactions par genre\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.boxplot(x=\"Gender\", y=\"Purchase\", data=train, ax=axes[1])\n",
    "axes[1].set_title(\"Purchase par genre\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achats par tranche d'âge\n",
    "age_order = [\"0-17\", \"18-25\", \"26-35\", \"36-45\", \"46-50\", \"51-55\", \"55+\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train[\"Age\"].value_counts().reindex(age_order).plot(kind=\"bar\", ax=axes[0], color=\"teal\")\n",
    "axes[0].set_title(\"Transactions par tranche d'âge\")\n",
    "\n",
    "sns.boxplot(x=\"Age\", y=\"Purchase\", data=train, order=age_order, ax=axes[1])\n",
    "axes[1].set_title(\"Purchase par tranche d'âge\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achats par catégorie de ville\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train[\"City_Category\"].value_counts().sort_index().plot(kind=\"bar\", ax=axes[0], color=\"purple\")\n",
    "axes[0].set_title(\"Transactions par ville\")\n",
    "\n",
    "sns.boxplot(x=\"City_Category\", y=\"Purchase\", data=train, order=[\"A\", \"B\", \"C\"], ax=axes[1])\n",
    "axes[1].set_title(\"Purchase par catégorie de ville\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achats par situation matrimoniale et par Occupation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.boxplot(x=\"Marital_Status\", y=\"Purchase\", data=train, ax=axes[0])\n",
    "axes[0].set_title(\"Purchase par situation matrimoniale\")\n",
    "\n",
    "train.groupby(\"Occupation\")[\"Purchase\"].mean().sort_values().plot(kind=\"barh\", ax=axes[1], color=\"darkorange\")\n",
    "axes[1].set_title(\"Purchase moyen par Occupation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achats par Product_Category_1\n",
    "train.groupby(\"Product_Category_1\")[\"Purchase\"].mean().sort_values(ascending=False).plot(\n",
    "    kind=\"bar\", figsize=(12, 5), color=\"darkgreen\", edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Purchase moyen par Product_Category_1\")\n",
    "plt.ylabel(\"Purchase moyen\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation (variables numériques)\n",
    "num_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "corr = train[num_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Matrice de corrélation\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df):\n",
    "    \"\"\"Nettoyage commun appliqué à train et test.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Indicateurs de valeurs manquantes\n",
    "    df[\"PC2_missing\"] = df[\"Product_Category_2\"].isnull().astype(int)\n",
    "    df[\"PC3_missing\"] = df[\"Product_Category_3\"].isnull().astype(int)\n",
    "    \n",
    "    # Remplacement des NA\n",
    "    df[\"Product_Category_2\"] = df[\"Product_Category_2\"].fillna(-1).astype(int)\n",
    "    df[\"Product_Category_3\"] = df[\"Product_Category_3\"].fillna(-1).astype(int)\n",
    "    \n",
    "    # Stay_In_Current_City_Years : '4+' -> 4\n",
    "    df[\"Stay_In_Current_City_Years\"] = (\n",
    "        df[\"Stay_In_Current_City_Years\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\"+\", \"\", regex=False)\n",
    "        .astype(int)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_clean = clean(train)\n",
    "test_clean = clean(test)\n",
    "\n",
    "print(\"Valeurs manquantes après nettoyage :\")\n",
    "print(train_clean.isnull().sum().sum(), \"(train)\")\n",
    "print(test_clean.isnull().sum().sum(), \"(test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hypothèses formulées\n",
    "\n",
    "À partir de l'analyse exploratoire, nous formulons les hypothèses suivantes :\n",
    "\n",
    "1. **Les hommes dépensent en moyenne plus que les femmes** lors du Black Friday.\n",
    "2. **La tranche d'âge 26-35 ans** représente le segment le plus actif en termes de nombre de transactions.\n",
    "3. **La catégorie de produit (`Product_Category_1`)** est le facteur le plus influent sur le montant d'achat.\n",
    "4. **La ville de catégorie B** génère le plus grand nombre de transactions.\n",
    "5. **La situation matrimoniale** n'a qu'un faible impact sur le montant d'achat.\n",
    "6. **Les catégories de produit secondaires** (2 et 3) contiennent des valeurs manquantes structurelles : leur absence est elle-même informative.\n",
    "7. **La transformation log1p** de la cible devrait améliorer la convergence des modèles de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLS = [\"User_ID\", \"Product_ID\"]\n",
    "TARGET = \"Purchase\"\n",
    "\n",
    "def build_features(df):\n",
    "    \"\"\"Construit les features pour la modélisation.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Suppression des identifiants\n",
    "    df = df.drop(columns=[c for c in ID_COLS if c in df.columns])\n",
    "    \n",
    "    # Encodage binaire du genre\n",
    "    df[\"Gender\"] = df[\"Gender\"].map({\"F\": 0, \"M\": 1}).astype(int)\n",
    "    \n",
    "    # Variables catégorielles en string pour CatBoost\n",
    "    df[\"Age\"] = df[\"Age\"].astype(str)\n",
    "    df[\"City_Category\"] = df[\"City_Category\"].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Séparation features / cible\n",
    "y_train = np.log1p(train_clean[TARGET])\n",
    "X_train = build_features(train_clean.drop(columns=[TARGET]))\n",
    "X_test = build_features(test_clean)\n",
    "\n",
    "print(f\"X_train : {X_train.shape}\")\n",
    "print(f\"X_test  : {X_test.shape}\")\n",
    "print(f\"Features : {list(X_train.columns)}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices des colonnes catégorielles pour CatBoost\n",
    "cat_cols = [\"Age\", \"City_Category\"]\n",
    "cat_indices = [X_train.columns.get_loc(c) for c in cat_cols]\n",
    "print(f\"Indices catégoriels : {cat_indices} ({cat_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "def evaluate_cv(model, X, y, name, cat_features=None):\n",
    "    \"\"\"Cross-validation avec RMSE (calculé en espace original après expm1).\"\"\"\n",
    "    rmse_scores = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if cat_features is not None:\n",
    "            model.fit(X_tr, y_tr, cat_features=cat_features, verbose=0)\n",
    "        else:\n",
    "            model.fit(X_tr, y_tr)\n",
    "        \n",
    "        preds_log = model.predict(X_val)\n",
    "        preds = np.expm1(preds_log)\n",
    "        actual = np.expm1(y_val)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(actual, preds))\n",
    "        rmse_scores.append(rmse)\n",
    "    \n",
    "    mean_rmse = np.mean(rmse_scores)\n",
    "    std_rmse = np.std(rmse_scores)\n",
    "    print(f\"{name:30s} – RMSE : {mean_rmse:.2f} (+/- {std_rmse:.2f})\")\n",
    "    return mean_rmse, std_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les baselines, on a besoin de features numériques uniquement\n",
    "# On encode Age et City_Category en numérique pour Ridge/Dummy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train_num = X_train.copy()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X_train_num[col] = le.fit_transform(X_train_num[col])\n",
    "\n",
    "results = {}\n",
    "\n",
    "# DummyRegressor\n",
    "dummy = DummyRegressor(strategy=\"mean\")\n",
    "m, s = evaluate_cv(dummy, X_train_num, y_train, \"DummyRegressor (mean)\")\n",
    "results[\"DummyRegressor\"] = m\n",
    "\n",
    "# Ridge\n",
    "ridge = Ridge(alpha=1.0, random_state=SEED)\n",
    "m, s = evaluate_cv(ridge, X_train_num, y_train, \"Ridge\")\n",
    "results[\"Ridge\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Modèle CatBoost + Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "catboost_model = CatBoostRegressor(\n",
    "    iterations=3000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    early_stopping_rounds=100,\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "m, s = evaluate_cv(catboost_model, X_train, y_train, \"CatBoost\", cat_features=cat_indices)\n",
    "results[\"CatBoost\"] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Résultats & Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\", columns=[\"RMSE_CV\"])\n",
    "results_df = results_df.sort_values(\"RMSE_CV\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "results_df.plot(kind=\"barh\", legend=False, color=[\"green\" if i == results_df.index[0] else \"steelblue\" for i in results_df.index])\n",
    "plt.xlabel(\"RMSE (CV 5-fold)\")\n",
    "plt.title(\"Comparaison des modèles\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interprétabilité (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Entraînement sur tout le train pour SHAP\n",
    "final_cb = CatBoostRegressor(\n",
    "    iterations=3000,\n",
    "    learning_rate=0.05,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5,\n",
    "    loss_function=\"RMSE\",\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    ")\n",
    "final_cb.fit(X_train, y_train, cat_features=cat_indices)\n",
    "\n",
    "# SHAP values (échantillon pour performance)\n",
    "sample_size = min(2000, len(X_train))\n",
    "X_sample = X_train.sample(sample_size, random_state=SEED)\n",
    "\n",
    "explainer = shap.TreeExplainer(final_cb)\n",
    "shap_values = explainer.shap_values(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_sample, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance CatBoost\n",
    "feat_imp = pd.Series(final_cb.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "feat_imp.plot(kind=\"barh\", figsize=(10, 6), color=\"teal\")\n",
    "plt.title(\"Feature Importance (CatBoost)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Entraînement final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le modèle final_cb est déjà entraîné sur tout le train (cf. section SHAP)\n",
    "print(\"Modèle final CatBoost prêt.\")\n",
    "print(f\"Nombre d'itérations utilisées : {final_cb.tree_count_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prédictions sur test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction\n",
    "preds_log = final_cb.predict(X_test)\n",
    "preds = np.expm1(preds_log)\n",
    "\n",
    "# Création du fichier submission\n",
    "submission = pd.DataFrame({\n",
    "    \"User_ID\": test[\"User_ID\"],\n",
    "    \"Product_ID\": test[\"Product_ID\"],\n",
    "    \"Purchase\": preds,\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(f\"Submission sauvegardée : submission.csv ({len(submission)} lignes)\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des prédictions vs distribution train\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train[\"Purchase\"], bins=50, alpha=0.7, label=\"Train\", edgecolor=\"black\")\n",
    "axes[0].hist(preds, bins=50, alpha=0.5, label=\"Prédictions test\", edgecolor=\"black\", color=\"orange\")\n",
    "axes[0].legend()\n",
    "axes[0].set_title(\"Distribution Purchase : Train vs Prédictions\")\n",
    "\n",
    "axes[1].hist(preds, bins=50, alpha=0.7, color=\"orange\", edgecolor=\"black\")\n",
    "axes[1].set_title(\"Distribution des prédictions\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : Clustering clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Agrégats par utilisateur (pas de fuite : on utilise uniquement train)\n",
    "user_agg = train.groupby(\"User_ID\").agg(\n",
    "    purchase_mean=(\"Purchase\", \"mean\"),\n",
    "    purchase_std=(\"Purchase\", \"std\"),\n",
    "    purchase_count=(\"Purchase\", \"count\"),\n",
    "    n_products=(\"Product_ID\", \"nunique\"),\n",
    "    n_categories=(\"Product_Category_1\", \"nunique\"),\n",
    ").reset_index()\n",
    "\n",
    "user_agg[\"purchase_std\"] = user_agg[\"purchase_std\"].fillna(0)\n",
    "\n",
    "# Normalisation\n",
    "features_cluster = [\"purchase_mean\", \"purchase_std\", \"purchase_count\", \"n_products\", \"n_categories\"]\n",
    "scaler = StandardScaler()\n",
    "X_cluster = scaler.fit_transform(user_agg[features_cluster])\n",
    "\n",
    "print(f\"Nombre d'utilisateurs uniques : {len(user_agg)}\")\n",
    "user_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode du coude\n",
    "inertias = []\n",
    "K_range = range(2, 11)\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    km.fit(X_cluster)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(K_range, inertias, \"bo-\")\n",
    "plt.xlabel(\"Nombre de clusters (k)\")\n",
    "plt.ylabel(\"Inertie\")\n",
    "plt.title(\"Méthode du coude\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans avec k=4\n",
    "km = KMeans(n_clusters=4, random_state=SEED, n_init=10)\n",
    "user_agg[\"cluster\"] = km.fit_predict(X_cluster)\n",
    "\n",
    "# PCA pour visualisation 2D\n",
    "pca = PCA(n_components=2, random_state=SEED)\n",
    "X_pca = pca.fit_transform(X_cluster)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=user_agg[\"cluster\"], cmap=\"viridis\", alpha=0.5, s=10)\n",
    "plt.colorbar(scatter, label=\"Cluster\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "plt.title(\"Segmentation clients – KMeans (k=4) + PCA\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profil de chaque cluster\n",
    "cluster_profile = user_agg.groupby(\"cluster\")[features_cluster].mean().round(2)\n",
    "cluster_profile[\"count\"] = user_agg.groupby(\"cluster\").size().values\n",
    "print(\"Profil moyen par cluster :\")\n",
    "cluster_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion & Perspectives\n",
    "\n",
    "### Résultats\n",
    "\n",
    "- **CatBoost** surpasse significativement les baselines (DummyRegressor, Ridge) en termes de RMSE.\n",
    "- La **catégorie de produit** (`Product_Category_1`) est la feature la plus importante pour prédire le montant d'achat.\n",
    "- La **transformation log1p** permet de stabiliser la distribution de la cible.\n",
    "- Le **clustering** révèle des segments de clients distincts (gros acheteurs, acheteurs occasionnels, etc.).\n",
    "\n",
    "### Perspectives d'amélioration\n",
    "\n",
    "1. **Tuning des hyper-paramètres** : Optuna ou recherche bayésienne pour optimiser CatBoost.\n",
    "2. **Stacking / Blending** : combiner CatBoost avec LightGBM ou XGBoost.\n",
    "3. **Features avancées** : agrégats utilisateur/produit calculés en cross-validation (target encoding sans fuite).\n",
    "4. **Données temporelles** : si des données temporelles étaient disponibles, exploiter les tendances.\n",
    "5. **Deep Learning** : réseaux d'embedding pour User/Product (approach collaborative filtering).\n",
    "\n",
    "### Choix méthodologiques\n",
    "\n",
    "- **Régression** (et non classification) : la variable cible `Purchase` est continue.\n",
    "- **CatBoost** : choisi pour sa gestion native des variables catégorielles et ses performances sur données tabulaires.\n",
    "- **Clustering non supervisé** : KMeans pour la segmentation exploratoire des profils clients.\n",
    "- **Aucune fuite de données** : pas d'agrégation sur la cible hors cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
